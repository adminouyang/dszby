import os
import re
import requests
import time
import json
import concurrent.futures
import random
import threading
from datetime import datetime, timedelta
from urllib.parse import quote, unquote
import base64
from queue import Queue
import eventlet
import sys
import configparser

# å¢åŠ é€’å½’æ·±åº¦é™åˆ¶
sys.setrecursionlimit(10000)

# ===============================
# é…ç½®åŒº
# ===============================

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = "Hotel/config.ini"
COOKIE_FILE = "Hotel/cookie.txt"

# å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–FOFA Cookie
def load_fofa_cookie():
    """ä»é…ç½®æ–‡ä»¶åŠ è½½FOFA Cookie"""
    cookie = ""
    
    # é¦–å…ˆå°è¯•ä»cookie.txtæ–‡ä»¶è¯»å–
    if os.path.exists(COOKIE_FILE):
        try:
            with open(COOKIE_FILE, 'r', encoding='utf-8') as f:
                cookie = f.read().strip()
            if cookie:
                print("âœ… ä»cookie.txtæ–‡ä»¶åŠ è½½FOFA CookieæˆåŠŸ")
                return cookie
        except Exception as e:
            print(f"âŒ è¯»å–cookie.txtæ–‡ä»¶å¤±è´¥: {e}")
    
    # ç„¶åå°è¯•ä»config.iniæ–‡ä»¶è¯»å–
    if os.path.exists(CONFIG_FILE):
        try:
            config = configparser.ConfigParser()
            config.read(CONFIG_FILE, encoding='utf-8')
            if config.has_section('FOFA') and config.has_option('FOFA', 'cookie'):
                cookie = config.get('FOFA', 'cookie')
                if cookie:
                    print("âœ… ä»config.iniæ–‡ä»¶åŠ è½½FOFA CookieæˆåŠŸ")
                    return cookie
        except Exception as e:
            print(f"âŒ è¯»å–config.iniæ–‡ä»¶å¤±è´¥: {e}")
    
    # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤Cookieï¼ˆå¯èƒ½éœ€è¦æ›´æ–°ï¼‰
    default_cookie = """isRedirectLang=1; is_mobile=pc; __fcd=DQVA3CHUNOEWDZUY01EE1FAF708C52E9; Hm_lvt_4275507ba9b9ea6b942c7a3f7c66da90=1769490368; HMACCOUNT=79E7429D30B36B70; _ga=GA1.1.561856398.1769490368; befor_router=%2Fresult%3Fqbase64%3DImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHByb3ZpbmNlOiAiYW5odWki%26page%3D5%26page_size%3D20; fofa_token=eyJhbGciOiJIUzUxMiIsImtpZCI6Ik5XWTVZakF4TVRkalltSTJNRFZsWXpRM05EWXdaakF3TURVMlkyWTNZemd3TUdRd1pUTmpZUT09IiwidHlwIjoiSldUIn0.eyJpZCI6MTE4MTAxMSwibWlkIjoxMDA3NDEyMzIsInVzZXJuYW1lIjoiT1VfeWFuZyIsInBhcmVudF9pZCI6MCwiZXhwIjoxNzcwMDk1MTk2fQ.Dgvo38VAYRzhoBjLlNdk9oAAwXczhGHjDALiJoleKcnMQDex9Mz6jDCOmpl-Ay5abNuGjlLxF8A1fTYMgmXPsA; user=%7B%22id%22%3A1181011%2C%22mid%22%3A100741232%2C%22is_admin%22%3Afalse%2C%22username%22%3A%22OU_yang%22%2C%22nickname%22%3A%22OU_yang%22%2C%22email%22%3A%222856364053%40qq.com%22%2C%22avatar_medium%22%3A%22https%3A%2F%2Fnosec.org%2Fmissing.jpg%22%2C%22avatar_thumb%22%3A%22https%3A%2F%2Fnosec.org%2Fmissing.jpg%22%2C%22key%22%3A%229495b90e65813ae0e9188e6a5928d1f1%22%2C%22category%22%3A%22user%22%2C%22rank_avatar%22%3A%22%22%2C%22rank_level%22%3A0%2C%22rank_name%22%3A%22%E6%B3%A8%E5%86%8C%E7%94%A8%E6%88%B7%22%2C%22company_name%22%3A%22OU_yang%22%2C%22coins%22%3A0%2C%22can_pay_coins%22%3A0%2C%22fofa_point%22%3A0%2C%22credits%22%3A1%2C%22expiration%22%3A%22-%22%2C%22login_at%22%3A0%2C%22data_limit%22%3A%7B%22web_query%22%3A300%2C%22web_data%22%3A3000%2C%22api_query%22%3A0%2C%22api_data%22%3A0%2C%22data%22%3A-1%2C%22query%22%3A-1%7D%2C%22expiration_notice%22%3Afalse%2C%22remain_giveaway%22%3A1000%2C%22fpoint_upgrade%22%3Afalse%2C%22account_status%22%3A%22%22%2C%22parents_id%22%3A0%2C%22parents_email%22%3A%22%22%2C%22parents_fpoint%22%3A0%2C%22created_at%22%3A%222026-01-25%2000%3A00%3A00%22%7D; is_flag_login=1; baseShowChange=false; viewOneHundredData=false; _ga_9GWBD260K9=GS2.1.s1769520942$o5$g1$t1769521320$j33$l0$h0; Hm_lpvt_4275507ba9b9ea6b942c7a3f7c66da90=1769521320"""
    
    print("âš ï¸ ä½¿ç”¨é»˜è®¤FOFA Cookieï¼Œå»ºè®®æ›´æ–°ä¸ºæœ‰æ•ˆçš„Cookie")
    return default_cookie

# åŠ è½½FOFA Cookie
FOFA_COOKIE = load_fofa_cookie()

# æœç´¢å…³é”®è¯
SEARCH_QUERIES = [
    '"iptv/live/zh_cn.js" && country="CN"',
    # å…¶ä»–æœç´¢æŸ¥è¯¢ä¿æŒä¸å˜...
]

# IPå­˜å‚¨ç›®å½•
IP_DIR = "Hotel/ip"
if not os.path.exists(IP_DIR):
    os.makedirs(IP_DIR)

# ç¡®ä¿Hotelç›®å½•å­˜åœ¨
if not os.path.exists("Hotel"):
    os.makedirs("Hotel")

# æµ‹é€Ÿé˜ˆå€¼ (MB/s)
SPEED_THRESHOLD = 0.1

# User-Agentåˆ—è¡¨
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
]

# å…¶ä»–é…ç½®ä¿æŒä¸å˜...
# ===============================
# æ–°å¢ï¼šHTMLä¹±ç å¤„ç†åŠŸèƒ½
# ===============================

def detect_encoding(content):
    """æ£€æµ‹HTMLå†…å®¹çš„ç¼–ç  - ä¿®å¤å­—èŠ‚/å­—ç¬¦ä¸²ç±»å‹é—®é¢˜"""
    # å¦‚æœå†…å®¹æ˜¯å­—èŠ‚ç±»å‹ï¼Œå…ˆå°è¯•è§£ç ä¸ºå­—ç¬¦ä¸²
    if isinstance(content, bytes):
        # å°è¯•å¸¸è§ç¼–ç è§£ç 
        encodings_to_try = ['utf-8', 'gbk', 'gb2312', 'iso-8859-1', 'big5']
        for encoding in encodings_to_try:
            try:
                content = content.decode(encoding, errors='ignore')
                break
            except:
                continue
        else:
            # æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è§£ç 
            content = content.decode('utf-8', errors='ignore')
    
    # ç°åœ¨contentç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
    encoding_patterns = [
        r'<meta[^>]*charset=["\']?([^"\'/>]+)',
        r'<meta[^>]*content=["\'][^"\'"]*charset=([^"\'"/>]+)',
        r'<\\?xml[^>]*encoding=["\']?([^"\'/>]+)'
    ]
    
    for pattern in encoding_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            encoding = match.group(1).lower()
            encoding_map = {
                'utf-8': 'utf-8', 'gbk': 'gbk', 'gb2312': 'gb2312',
                'iso-8859-1': 'iso-8859-1', 'big5': 'big5'
            }
            if encoding in encoding_map:
                return encoding_map[encoding]
    
    return 'utf-8'

def fix_html_encoding(response):
    """ä¿®å¤HTMLå“åº”ç¼–ç  - ä¿®å¤ç‰ˆæœ¬"""
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨requestsçš„è‡ªåŠ¨æ£€æµ‹
        if response.encoding:
            try:
                decoded_content = response.content.decode(response.encoding)
                return decoded_content
            except:
                pass
        
        # ä½¿ç”¨ä¿®å¤åçš„ç¼–ç æ£€æµ‹
        detected_encoding = detect_encoding(response.content)
        try:
            decoded_content = response.content.decode(detected_encoding, errors='ignore')
            return decoded_content
        except:
            # å›é€€æ–¹æ¡ˆ
            decoded_content = response.content.decode('utf-8', errors='ignore')
            return decoded_content
            
    except Exception as e:
        print(f"âŒ HTMLç¼–ç ä¿®å¤å¤±è´¥: {e}")
        # æœ€ç»ˆå›é€€
        return response.content.decode('iso-8859-1', errors='ignore')
        
def save_html_with_fixed_encoding(filename, content, original_encoding='utf-8'):
    """ä¿å­˜ä¿®å¤ç¼–ç åçš„HTMLå†…å®¹"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"ğŸ’¾ å·²ä¿å­˜ä¿®å¤ç¼–ç çš„HTMLåˆ° {filename}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜HTMLæ–‡ä»¶å¤±è´¥: {e}")
        # å°è¯•ä½¿ç”¨åŸå§‹ç¼–ç ä¿å­˜
        try:
            with open(filename, 'w', encoding=original_encoding, errors='ignore') as f:
                f.write(content)
            print(f"ğŸ’¾ å·²ä½¿ç”¨åŸå§‹ç¼–ç ä¿å­˜HTMLåˆ° {filename}")
            return True
        except:
            print(f"âŒ ä½¿ç”¨åŸå§‹ç¼–ç ä¿å­˜ä¹Ÿå¤±è´¥")
            return False

# ===============================
# ä¿®æ”¹çˆ¬å–å‡½æ•°ï¼Œå¢åŠ ç¼–ç å¤„ç†
# ===============================

def crawl_fofa_with_cookie():
    """ä½¿ç”¨Cookieçˆ¬å–FOFAæ•°æ® - å¢åŠ ç¼–ç å¤„ç†"""
    urls = generate_fofa_urls()
    all_ips = set()
    session = requests.Session()
    
    print(f"ğŸ” å¼€å§‹çˆ¬å–FOFAï¼Œå…± {len(urls)} ä¸ªæœç´¢é¡µé¢")
    
    for i, url in enumerate(urls, 1):
        print(f"ğŸ“¡ æ­£åœ¨çˆ¬å–ç¬¬ {i}/{len(urls)} é¡µ: {url}")
        
        try:
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(random.uniform(2, 5))
            
            # ä½¿ç”¨å¸¦Cookieçš„headers
            headers = get_random_headers()
            response = session.get(url, headers=headers, timeout=15)
            
            if response.status_code == 403 or "è®¿é—®é™åˆ¶" in response.text or "è¯·ç™»å½•" in response.text:
                print(f"âŒ ç¬¬ {i} é¡µè®¿é—®è¢«é™åˆ¶ï¼Œå¯èƒ½éœ€è¦é‡æ–°ç™»å½•")
                continue
            
            if response.status_code != 200:
                print(f"âŒ ç¬¬ {i} é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                continue
            
            # ä¿®å¤HTMLç¼–ç 
            fixed_content = fix_html_encoding(response)
            
            # ä¿å­˜ä¿®å¤åçš„é¡µé¢å†…å®¹ç”¨äºåˆ†æ
            if i == 1:  # åªä¿å­˜ç¬¬ä¸€é¡µç”¨äºè°ƒè¯•
                save_html_with_fixed_encoding("fofa_first_page_fixed.html", fixed_content, response.encoding)
                print("ğŸ’¾ å·²ä¿å­˜ä¿®å¤ç¼–ç çš„ç¬¬ä¸€é¡µHTMLåˆ° fofa_first_page_fixed.html")
            
            # å¤šç§æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…IP
            ip_patterns = [
                r'<a[^>]*href="[^"]*?//(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{1,5})"',
                r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{1,5})',
                r'ip.*?(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}).*?port.*?(\d{1,5})',
                r'host.*?(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}).*?port.*?(\d{1,5})'
            ]
            
            page_ips = set()
            for pattern in ip_patterns:
                matches = re.findall(pattern, fixed_content, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        if len(match) == 2:
                            ip_port = f"{match[0]}:{match[1]}"
                        else:
                            continue
                    else:
                        ip_port = match
                    
                    # éªŒè¯IPå’Œç«¯å£æ ¼å¼
                    ip_match = re.match(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(\d{1,5})', ip_port)
                    if ip_match:
                        # éªŒè¯IPåœ°å€çš„æ¯ä¸ªéƒ¨åˆ†
                        ip_parts = ip_match.group(1).split('.')
                        if all(0 <= int(part) <= 255 for part in ip_parts):
                            # éªŒè¯ç«¯å£
                            port = int(ip_match.group(2))
                            if 1 <= port <= 65535:
                                page_ips.add(ip_port)
                                print(f"âœ… æ‰¾åˆ°IP: {ip_port}")
            
            all_ips.update(page_ips)
            print(f"âœ… ç¬¬ {i} é¡µè·å–åˆ° {len(page_ips)} ä¸ªIPï¼Œå½“å‰æ€»æ•° {len(all_ips)}")
            
        except Exception as e:
            print(f"âŒ ç¬¬ {i} é¡µçˆ¬å–å¤±è´¥: {e}")
    
    print(f"ğŸ¯ FOFAçˆ¬å–å®Œæˆï¼Œæ€»å…±è·å–åˆ° {len(all_ips)} ä¸ªæœ‰æ•ˆIP")
    return all_ips

# ===============================
# å·¥å…·å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
# ===============================

def get_random_headers():
    """è·å–éšæœºUser-Agentçš„headers"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Cookie": FOFA_COOKIE
    }

def get_isp(ip):
    """IPè¿è¥å•†åˆ¤æ–­"""
    telecom_pattern = r"^(1\.|14\.|27\.|36\.|39\.|42\.|49\.|58\.|60\.|101\.|106\.|110\.|111\.|112\.|113\.|114\.|115\.|116\.|117\.|118\.|119\.|120\.|121\.|122\.|123\.|124\.|125\.|126\.|171\.|175\.|182\.|183\.|202\.|203\.|210\.|211\.|218\.|219\.|220\.|221\.|222\.)"
    unicom_pattern = r"^(42\.1[0-9]{0,2}|43\.|58\.|59\.|60\.|61\.|110\.|111\.|112\.|113\.|114\.|115\.|116\.|117\.|118\.|119\.|120\.|121\.|122\.|123\.|124\.|125\.|126\.|171\.8[0-9]|171\.9[0-9]|171\.1[0-9]{2}|175\.|182\.|183\.|210\.|211\.|218\.|219\.|220\.|221\.|222\.)"
    mobile_pattern = r"^(36\.|37\.|38\.|39\.1[0-9]{0,2}|42\.2|42\.3|47\.|106\.|111\.|112\.|113\.|114\.|115\.|116\.|117\.|118\.|119\.|120\.|121\.|122\.|123\.|124\.|125\.|126\.|134\.|135\.|136\.|137\.|138\.|139\.|150\.|151\.|152\.|157\.|158\.|159\.|170\.|178\.|182\.|183\.|184\.|187\.|188\.|189\.)"
    
    if re.match(telecom_pattern, ip):
        return "ç”µä¿¡"
    elif re.match(unicom_pattern, ip):
        return "è”é€š"
    elif re.match(mobile_pattern, ip):
        return "ç§»åŠ¨"
    else:
        return "æœªçŸ¥"

def get_ip_info(ip_port):
    """è·å–IPåœ°ç†ä¿¡æ¯"""
    try:
        ip = ip_port.split(":")[0]
        
        # ä½¿ç”¨IP-APIæŸ¥è¯¢
        try:
            response = requests.get(f"http://ip-api.com/json/{ip}?lang=zh-CN", timeout=5)
            if response.status_code == 200:
                data = response.json()
                if data.get("status") == "success":
                    province = data.get("regionName", "æœªçŸ¥")
                    isp = get_isp(ip)
                    return province, isp, ip_port
        except:
            pass
        
        return "æœªçŸ¥", "æœªçŸ¥", ip_port
        
    except Exception as e:
        return "æœªçŸ¥", "æœªçŸ¥", ip_port

def parse_ip_line(line):
    """è§£æIPè¡Œï¼Œæ”¯æŒæ ¼å¼ï¼šip:port$è¿è¥å•†å·²å­˜æ´»nå¤©"""
    line = line.strip()
    if not line or line.startswith('#'):
        return None, None, 0, None, None
    
    # åŒ¹é…IP:ç«¯å£æ ¼å¼
    ip_match = re.match(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{1,5})', line)
    if not ip_match:
        return None, None, 0, None, None
    
    ip_port = ip_match.group(1)
    
    # å°è¯•è§£æå­˜æ´»å¤©æ•°
    days_match = re.search(r'å·²å­˜æ´»(\d+)å¤©', line)
    days = int(days_match.group(1)) if days_match else 0
    
    # å°è¯•è§£æè¿è¥å•†
    isp_match = re.search(r'\$([^$]+?)å·²å­˜æ´»', line)
    isp = isp_match.group(1).strip() if isp_match else ""
    
    # å°è¯•è§£ææœ€åæ›´æ–°æ—¥æœŸ
    date_match = re.search(r'æœ€åæ›´æ–°:(\d{4}-\d{2}-\d{2})', line)
    last_update = date_match.group(1) if date_match else None
    
    # å°è¯•è§£æé€Ÿåº¦
    speed_match = re.search(r'#é€Ÿåº¦:([\d.]+)MB/s', line)
    speed = float(speed_match.group(1)) if speed_match else 0.0
    
    return ip_port, isp, days, last_update, speed

def read_existing_ips(filepath):
    """è¯»å–ç°æœ‰æ–‡ä»¶å†…å®¹å¹¶è§£æ"""
    existing_ips = {}  # ip_port: (days, isp, last_update, speed)
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    ip_port, isp, days, last_update, speed = parse_ip_line(line)
                    if ip_port:
                        existing_ips[ip_port] = (days, isp, last_update, speed)
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {filepath} å¤±è´¥: {e}")
    
    return existing_ips

def encode_query(query):
    """ç¼–ç æŸ¥è¯¢å­—ç¬¦ä¸²ä¸ºbase64"""
    return base64.b64encode(query.encode()).decode()

def generate_fofa_urls():
    """ç”ŸæˆFOFAæœç´¢URL"""
    urls = []
    pages = 2
    page_size = 20
    
    for query in SEARCH_QUERIES:
        encoded_query = encode_query(query)
        for page in range(1, pages + 1):
            url = f"https://fofa.info/result?qbase64={encoded_query}&page={page}&page_size={page_size}"
            urls.append(url)
    
    return urls

# ===============================
# IPå¯ç”¨æ€§éªŒè¯å’Œæµ‹é€Ÿå‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
# ===============================

def test_ip_availability(ip_port):
    """æµ‹è¯•IPå¯ç”¨æ€§"""
    try:
        # æµ‹è¯•JSONæ¥å£
        json_url = f"http://{ip_port}/iptv/live/1000.json?key=txiptv"
        response = requests.get(json_url, timeout=5)
        
        if response.status_code == 200:
            try:
                data = response.json()
                if data.get("code") == 0 and "data" in data:
                    return True, data
            except:
                pass
        return False, None
    except:
        return False, None

def get_province_tv_url(ip_port, json_data, province_name):
    """è·å–çœä»½å«è§†URL"""
    try:
        tv_name = None
        # è¿™é‡Œéœ€è¦å®šä¹‰PROVINCE_TV_MAPï¼Œå‡è®¾å·²ç»å®šä¹‰
        for channel in PROVINCE_TV_MAP.values():
            if province_name in channel:
                tv_name = channel
                break
        
        if not tv_name:
            tv_name = f"{province_name}å«è§†"
        
        for channel in json_data.get("data", []):
            channel_name = channel.get("name", "")
            if tv_name in channel_name:
                url = channel.get("url", "")
                if url:
                    if url.startswith("/"):
                        return f"http://{ip_port}{url}"
                    else:
                        return f"http://{ip_port}/{url}"
        return None
    except:
        return None

def test_channel_speed(channel_url, max_attempts=2):
    """æµ‹è¯•é¢‘é“é€Ÿåº¦"""
    best_speed = 0.0
    
    for attempt in range(max_attempts):
        try:
            # è·å–m3u8æ–‡ä»¶å†…å®¹
            response = requests.get(channel_url, timeout=3)
            if response.status_code != 200:
                if attempt < max_attempts - 1:
                    print(f"ç¬¬{attempt+1}æ¬¡æµ‹é€Ÿ {channel_url}: HTTP {response.status_code}ï¼Œå°†é‡è¯•")
                continue
                
            lines = response.text.strip().split('\n')
            ts_lists = [line.split('/')[-1] for line in lines if line.startswith('#') == False and line.strip()]
            if not ts_lists:
                if attempt < max_attempts - 1:
                    print(f"ç¬¬{attempt+1}æ¬¡æµ‹é€Ÿ {channel_url}: æ²¡æœ‰æ‰¾åˆ°TSåˆ—è¡¨ï¼Œå°†é‡è¯•")
                continue
            
            # è·å–TSæ–‡ä»¶çš„URL
            channel_url_t = channel_url.rstrip(channel_url.split('/')[-1])
            ts_url = channel_url_t + ts_lists[0]
            
            # æµ‹é€Ÿé€»è¾‘
            start_time = time.time()
            try:
                with eventlet.Timeout(5, False):
                    ts_response = requests.get(ts_url, timeout=6, stream=True)
                    if ts_response.status_code != 200:
                        if attempt < max_attempts - 1:
                            print(f"ç¬¬{attempt+1}æ¬¡æµ‹é€Ÿ {channel_url}: TSæ–‡ä»¶HTTP {ts_response.status_code}ï¼Œå°†é‡è¯•")
                        continue
                    
                    # è¯»å–éƒ¨åˆ†å†…å®¹è¿›è¡Œæµ‹é€Ÿ
                    content_length = 0
                    chunk_size = 1024 * 1024  # 1MB
                    for chunk in ts_response.iter_content(chunk_size=chunk_size):
                        if chunk:
                            content_length += len(chunk)
                            # åªè¯»å–1MBç”¨äºæµ‹é€Ÿ
                            if content_length >= chunk_size:
                                break
                    
                    resp_time = time.time() - start_time
                    
                    if content_length > 0 and resp_time > 0:
                        normalized_speed = content_length / resp_time / 1024 / 1024
                        
                        # æ›´æ–°æœ€ä½³é€Ÿåº¦
                        if normalized_speed > best_speed:
                            best_speed = normalized_speed
                        
                        # å¦‚æœé€Ÿåº¦åˆæ ¼ï¼Œä¸å†é‡è¯•
                        if normalized_speed > SPEED_THRESHOLD:
                            break
                        else:
                            if attempt < max_attempts - 1:
                                print(f"ç¬¬{attempt+1}æ¬¡æµ‹é€Ÿ {channel_url}: {normalized_speed:.3f} MB/sï¼Œå°†é‡è¯•")
                    else:
                        if attempt < max_attempts - 1:
                            print(f"ç¬¬{attempt+1}æ¬¡æµ‹é€Ÿ {channel_url}: è·å–å†…å®¹å¤±è´¥ï¼Œå°†é‡è¯•")
            except eventlet.Timeout:
                if attempt < max_attempts - 1:
                    print(f"ç¬¬{attempt+1}æ¬¡æµ‹é€Ÿ {channel_url}: è¯·æ±‚è¶…æ—¶ï¼Œå°†é‡è¯•")
                continue
            except Exception as e:
                if attempt < max_attempts - 1:
                    print(f"ç¬¬{attempt+1}æ¬¡æµ‹é€Ÿ {channel_url} å¤±è´¥: {str(e)}ï¼Œå°†é‡è¯•")
                continue
                
        except Exception as e:
            if attempt < max_attempts - 1:
                print(f"ç¬¬{attempt+1}æ¬¡æµ‹é€Ÿ {channel_url} å¤„ç†å¤±è´¥: {str(e)}ï¼Œå°†é‡è¯•")
            continue
    
    return best_speed

def test_single_ip(ip_port, province_name):
    """æµ‹è¯•å•ä¸ªIPçš„å¯ç”¨æ€§å’Œé€Ÿåº¦"""
    try:
        # 1. æµ‹è¯•IPå¯ç”¨æ€§
        is_available, json_data = test_ip_availability(ip_port)
        if not is_available:
            return 0.0, False
        
        # 2. è·å–çœä»½å«è§†URL
        channel_url = get_province_tv_url(ip_port, json_data, province_name)
        if not channel_url:
            return 0.0, False
        
        # 3. æµ‹é€Ÿ
        speed = test_channel_speed(channel_url)
        return speed, speed > SPEED_THRESHOLD
        
    except Exception as e:
        return 0.0, False

def speed_test_ips(ip_list, province_name):
    """å¤šçº¿ç¨‹æµ‹é€ŸIPåˆ—è¡¨"""
    results = []
    checked = [0]
    total_count = len(ip_list)
    
    def show_progress():
        """æ˜¾ç¤ºè¿›åº¦"""
        while checked[0] < total_count:
            numberx = checked[0] / total_count * 100
            print(f"å·²æµ‹è¯•{checked[0]}/{total_count}ï¼Œå¯ç”¨é¢‘é“:{len(results)}ä¸ªï¼Œè¿›åº¦:{numberx:.2f}%")
            time.sleep(5)
    
    def worker():
        """å·¥ä½œçº¿ç¨‹"""
        while True:
            try:
                # ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡
                with task_queue_lock:
                    if not task_queue:
                        break
                    ip_info = task_queue.pop(0)
                
                ip_port = ip_info[0]
                speed, is_usable = test_single_ip(ip_port, province_name)
                
                if is_usable:
                    result = (ip_info[0], ip_info[1], ip_info[2], speed)
                    results.append(result)
                    print(f"âœ“ {ip_port}: {speed:.3f} MB/s")
                else:
                    print(f"Ã— {ip_port}: {speed:.3f} MB/s")
                
                checked[0] += 1
            except Exception as e:
                checked[0] += 1
                print(f"å¤„ç† {ip_info[0]} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    task_queue = ip_list.copy()
    task_queue_lock = threading.Lock()
    
    # å¯åŠ¨è¿›åº¦æ˜¾ç¤ºçº¿ç¨‹
    progress_thread = threading.Thread(target=show_progress, daemon=True)
    progress_thread.start()
    
    # åˆ›å»ºå·¥ä½œçº¿ç¨‹
    threads = []
    for _ in range(min(10, len(ip_list))):
        thread = threading.Thread(target=worker)
        thread.daemon = True
        thread.start()
        threads.append(thread)
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join()
    
    # æŒ‰é€Ÿåº¦æ’åº
    results.sort(key=lambda x: x[3], reverse=True)
    return results

# ===============================
# æ–‡ä»¶ç®¡ç†å’Œæ›´æ–°å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
# ===============================

def calculate_days_between(date_str1, date_str2):
    """è®¡ç®—ä¸¤ä¸ªæ—¥æœŸå­—ç¬¦ä¸²ä¹‹é—´çš„å¤©æ•°å·®"""
    try:
        date1 = datetime.strptime(date_str1, "%Y-%m-%d")
        date2 = datetime.strptime(date_str2, "%Y-%m-%d")
        return (date2 - date1).days
    except:
        return 0

def update_ip_file(filepath, new_usable_ips):
    """æ›´æ–°IPæ–‡ä»¶ - ä¿®å¤å­˜æ´»å¤©æ•°è®¡ç®—"""
    try:
        # è¯»å–ç°æœ‰IP
        existing_ips = read_existing_ips(filepath)
        
        # æ›´æ–°å­˜æ´»å¤©æ•°
        updated_ips = {}
        for ip_port, (days, isp, last_update, old_speed) in existing_ips.items():
            # æ£€æŸ¥IPæ˜¯å¦åœ¨æ–°å¯ç”¨åˆ—è¡¨ä¸­
            is_still_usable = any(ip[0] == ip_port for ip in new_usable_ips)
            if is_still_usable:
                # å¦‚æœè¿è¥å•†ä¸ºç©ºï¼Œå°è¯•è·å–
                if not isp:
                    ip = ip_port.split(":")[0]
                    isp = get_isp(ip)
                updated_ips[ip_port] = (days + 1, isp, last_update, old_speed)
            # å¦‚æœä¸åœ¨æ–°åˆ—è¡¨ä¸­ä½†åŸæ¥å¯ç”¨ï¼Œä¿æŒåŸæ ·
            elif days > 0:
                updated_ips[ip_port] = (days, isp, last_update, old_speed)
        
        # æ·»åŠ æ–°IP
        for ip_info in new_usable_ips:
            ip_port, isp, old_days, speed = ip_info
            if ip_port not in updated_ips:
                # ç¡®ä¿è¿è¥å•†ä¸ä¸ºç©º
                if not isp:
                    ip = ip_port.split(":")[0]
                    isp = get_isp(ip)
                updated_ips[ip_port] = (1, isp, datetime.now().strftime("%Y-%m-%d"), speed)  # æ–°IPå­˜æ´»å¤©æ•°ä¸º1
        
        # å¦‚æœæ–‡ä»¶ä¸ºç©ºï¼Œåˆ é™¤æ–‡ä»¶
        if not updated_ips:
            if os.path.exists(filepath):
                os.remove(filepath)
            print(f"ğŸ—‘ï¸ åˆ é™¤ç©ºæ–‡ä»¶: {os.path.basename(filepath)}")
            return
        
        # å†™å…¥æ–‡ä»¶ - ä¿®å¤æ ¼å¼
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# æµ‹é€Ÿé˜ˆå€¼: {SPEED_THRESHOLD} MB/s\n")
            f.write("# æ ¼å¼: IP:ç«¯å£$è¿è¥å•†å·²å­˜æ´»nå¤©#æœ€åæ›´æ–°:YYYY-MM-DD#é€Ÿåº¦\n")
            f.write("=" * 50 + "\n")
            
            # æŒ‰å­˜æ´»å¤©æ•°æ’åº
            sorted_ips = sorted(updated_ips.items(), key=lambda x: x[1][0], reverse=True)
            
            for ip_port, (days, isp, last_update, speed) in sorted_ips:
                # æŸ¥æ‰¾å¯¹åº”çš„é€Ÿåº¦ä¿¡æ¯
                speed_info = f"#é€Ÿåº¦:{speed:.3f}MB/s" if speed > 0 else ""
                f.write(f"{ip_port}${isp}å·²å­˜æ´»{days}å¤©#æœ€åæ›´æ–°:{last_update}{speed_info}\n")
        
        print(f"ğŸ’¾ å·²æ›´æ–° {os.path.basename(filepath)}ï¼Œæœ‰æ•ˆIP: {len(updated_ips)} ä¸ª")
        
    except Exception as e:
        print(f"âŒ æ›´æ–°æ–‡ä»¶ {filepath} å¤±è´¥: {e}")

def validate_existing_ips():
    """éªŒè¯ç°æœ‰IPæ–‡ä»¶ä¸­çš„IP"""
    print("ğŸ” å¼€å§‹éªŒè¯ç°æœ‰IPæ–‡ä»¶...")
    
    for filename in os.listdir(IP_DIR):
        if filename.endswith('.txt') and filename != "ip_summary.txt":
            filepath = os.path.join(IP_DIR, filename)
            
            # ä»æ–‡ä»¶åæå–çœä»½å’Œè¿è¥å•†
            match = re.match(r'(.+?)(ç”µä¿¡|è”é€š|ç§»åŠ¨|æœªçŸ¥)\.txt', filename)
            if not match:
                continue
                
            province = match.group(1)
            isp = match.group(2)
            
            print(f"ğŸ“‹ éªŒè¯æ–‡ä»¶: {filename} (çœä»½: {province}, è¿è¥å•†: {isp})")
            
            # è¯»å–IP
            existing_ips = read_existing_ips(filepath)
            if not existing_ips:
                print(f"âš ï¸ æ–‡ä»¶ {filename} ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯")
                continue
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            ip_list = []
            for ip_port, (days, isp_val, last_update, speed) in existing_ips.items():
                ip_list.append((ip_port, isp_val, days))
            
            # æµ‹è¯•IP
            usable_ips = speed_test_ips(ip_list, province)
            
            # æ›´æ–°æ–‡ä»¶
            update_ip_file(filepath, usable_ips)
    
    print("âœ… ç°æœ‰IPéªŒè¯å®Œæˆ")

def process_new_ips(new_ips):
    """å¤„ç†æ–°è·å–çš„IP - ä¿®å¤è¿è¥å•†è·å–"""
    if not new_ips:
        print("âš ï¸ æ²¡æœ‰è·å–åˆ°æ–°IP")
        return
    
    print(f"ğŸ”§ å¼€å§‹å¤„ç† {len(new_ips)} ä¸ªæ–°IP...")
    
    # è·å–IPä¿¡æ¯
    province_isp_dict = {}
    
    # ä½¿ç”¨çº¿ç¨‹æ± è·å–IPä¿¡æ¯
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_ip = {executor.submit(get_ip_info, ip): ip for ip in new_ips}
        
        for future in concurrent.futures.as_completed(future_to_ip):
            province, isp, ip_port = future.result()
            
            # ç¡®ä¿çœä»½å’Œè¿è¥å•†ä¸ä¸ºç©º
            if not province or province == "æœªçŸ¥":
                province = "å…¶ä»–"
            else:
                # æ¸…ç†çœä»½åç§°
                province = province.replace("çœ", "").replace("å¸‚", "").replace("è‡ªæ²»åŒº", "").replace("ç‰¹åˆ«è¡Œæ”¿åŒº", "").strip()
                if not province:
                    province = "å…¶ä»–"
            
            if not isp or isp == "æœªçŸ¥":
                ip = ip_port.split(":")[0]
                isp = get_isp(ip)
            
            fname = f"{province}{isp}.txt"
            province_isp_dict.setdefault(fname, []).append((ip_port, isp, 0))  # æ–°IPå­˜æ´»å¤©æ•°ä¸º0
    
    # æµ‹è¯•å¹¶ä¿å­˜æ–°IP
    for fname, ip_list in province_isp_dict.items():
        filepath = os.path.join(IP_DIR, fname)
        
        # ä»æ–‡ä»¶åæå–çœä»½
        match = re.match(r'(.+?)(ç”µä¿¡|è”é€š|ç§»åŠ¨|æœªçŸ¥)\.txt', fname)
        province = match.group(1) if match else "å…¶ä»–"
        
        print(f"ğŸ§ª æµ‹è¯• {fname} ä¸­çš„ {len(ip_list)} ä¸ªæ–°IP...")
        usable_ips = speed_test_ips(ip_list, province)
        
        if usable_ips:
            update_ip_file(filepath, usable_ips)
        else:
            print(f"âš ï¸ {fname} ä¸­æ²¡æœ‰å¯ç”¨çš„æ–°IP")
    
    print("âœ… æ–°IPå¤„ç†å®Œæˆ")

# ===============================
# ä¸»å‡½æ•°
# ===============================

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸŒ FOFA IPåœ°å€æŠ“å–ä¸éªŒè¯å·¥å…·")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {IP_DIR}")
    print(f"âš¡ æµ‹é€Ÿé˜ˆå€¼: {SPEED_THRESHOLD} MB/s")
    print("=" * 60)
    
    # ç¬¬ä¸€é˜¶æ®µï¼šéªŒè¯ç°æœ‰IP
    validate_existing_ips()
    
    # ç¬¬äºŒé˜¶æ®µï¼šçˆ¬å–æ–°IP
    print("\nğŸš€ å¼€å§‹çˆ¬å–FOFAæ–°IP...")
    new_ips = crawl_fofa_with_cookie()
    
    if new_ips:
        # å¤„ç†æ–°IP
        process_new_ips(new_ips)
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°æ–°IP")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    # å®‰è£…ä¾èµ–: pip install eventlet
    main()
